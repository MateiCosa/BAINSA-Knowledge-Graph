{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2002e5b0",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9704de22",
   "metadata": {},
   "source": [
    "This notebook is part of the BAINSA Wikipedia Knowledge Graph Project. Using the Wiki Dumps, we aim to construct an unweighted directed graph containg the Wikipedia articles as nodes and page links as edges. In this particular notebook, we use the files \"articles.txt\" and \"article_links.txt\" that we have created based on data gathered from the Wiki dumps. The structure of \"articles.txt\" is: page_id page_title. The structure of \"article_links\" is: page_title page_id_1 page_id_2 ... page_id_n, where page_id_i represents the id of the ith page that contains the link to the current page. In other words, \"article.txt\" contains the nodes, while \"article_links\" is a collection of inward edges. The problem is that some of pages and ids in \"article_links\" correspond to improper pages (e.g., redirects). Therefore, we have to filter for only those titles that are present in both files, and then check that the ids of the links are present in the intersection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50aee500",
   "metadata": {},
   "source": [
    "## Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0548942b",
   "metadata": {},
   "source": [
    "We use dictionaries to store the data from the two files. The first one (ids_titles) uses the titles as keys and the ids as values, whereas the second one (all_titles) uses the titles as keys and the list of ids representing inward edges as values. What we have to do is find the \"intersection\" of the titles and then filter out the ids correspong to edges connecting nodes that are not in the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4886b31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31108e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_titles = dict()\n",
    "i = 0\n",
    "with open(\"/Users/mateicosa/Bocconi/BAINSA/articles.txt\", \"r\") as f:\n",
    "    line = f.readline()\n",
    "    while line:\n",
    "        split_line = line.split()\n",
    "        ids_titles[split_line[1][1:-1].replace(\"\\\\\", \"\")] = split_line[0]\n",
    "        line = f.readline()\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "79f1ddb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_titles = dict()\n",
    "i = 0 \n",
    "clean_titles = set(ids_titles.keys())\n",
    "with open(\"/Users/mateicosa/Bocconi/BAINSA/article_links.txt\", \"r\") as g:\n",
    "    line = g.readline()\n",
    "    while line:\n",
    "        split_line = line.split()\n",
    "        title = split_line[0][1:-1].replace(\"\\\\\", \"\")\n",
    "        link_ids = np.array(split_line[1:])\n",
    "        if title != \"\" and title in clean_titles:\n",
    "            all_titles[title] = link_ids\n",
    "            i += 1\n",
    "            clean_titles.remove(title)\n",
    "        line = g.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6a6e51f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6090843\n",
      "5957440\n"
     ]
    }
   ],
   "source": [
    "print(len(ids_titles))\n",
    "print(len(all_titles))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292f4af8",
   "metadata": {},
   "source": [
    "We notice that the first file contains more data than the second one, hence filtering is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0a8650d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del clean_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fb28b1",
   "metadata": {},
   "source": [
    "## Filtering the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec23be70",
   "metadata": {},
   "source": [
    "We create a dictionary containing the keys of all_titles to speed up the search process. In the end, we want to produce 2 output files: \"final_articles.txt\" and \"inward_edges.txt\". Our aim is for \"final_articles.txt\" to contain the page_id followed by the page_title, while \"inward_edges.txt\" to contain the page_id, followed by the ids of the pages from which the page can be reached. We proceed with the creation of the first file, while keeping track of the titles and ids that are missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a3a71df",
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_titles = set(all_titles.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b923c7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open(\"final_articles.txt\", \"w\")\n",
    "missing_titles = []\n",
    "missing_ids = []\n",
    "valid_ids = set()\n",
    "for title in ids_titles.keys():\n",
    "    if title in remaining_titles:\n",
    "        output.write(ids_titles[title] + \" \" + title + '\\n')\n",
    "        remaining_titles.remove(title)\n",
    "        valid_ids.add(ids_titles[title])\n",
    "    else:\n",
    "        missing_titles.append(title)\n",
    "        missing_ids.append(ids_titles[title])\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "395d0da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133403 133403 5957440\n"
     ]
    }
   ],
   "source": [
    "print(len(missing_titles), len(missing_ids), len(all_titles))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cd08c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_titles = set(missing_titles)\n",
    "missing_ids = set(missing_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fd2820",
   "metadata": {},
   "source": [
    "We now delete the keys that belong to only one of the dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96ff6f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "for title in missing_titles:\n",
    "    if title in all_titles.keys():\n",
    "        del all_titles[title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "96eb16e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5957440\n"
     ]
    }
   ],
   "source": [
    "print(len(all_titles))\n",
    "del missing_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8705f98",
   "metadata": {},
   "source": [
    "Finally, we create the last file by cross-checing the page ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0f2b1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = open(\"inward_edges.txt\", \"w\")\n",
    "for title in all_titles.keys():\n",
    "    output.write(ids_titles[title])\n",
    "    for page_id in all_titles[title]:\n",
    "        if page_id in valid_ids:\n",
    "            output.write(' ' + page_id)\n",
    "    output.write('\\n')\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29939b99",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499d4a66",
   "metadata": {},
   "source": [
    "We now have all the \"ingredients\" to build the directed, unweighted graph of English Wikipedia."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
