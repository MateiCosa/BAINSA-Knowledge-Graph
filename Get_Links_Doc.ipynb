{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06d75f03",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6a0296",
   "metadata": {},
   "source": [
    "This notebook is part of the BAINSA Wikipedia Knowledge Graph Project. Using the Wiki Dumps, we aim to construct an unweighted directed graph containg the Wikipedia articles as nodes and page links as edges. In this particular notebook, we use the file \"enwiki-latest-pagelinks.sql\". To parse it, we exploit the structure of the SQL language as shown in the example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "846378c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/mateicosa/Downloads/enwiki-latest-pagelinks.sql\", \"r\") as f:\n",
    "    line = \"a\"\n",
    "    i = 0\n",
    "    while \"INSERT INTO\" not in line:\n",
    "        i += 1\n",
    "        line = f.readline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52f9aa3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(586,0,'!',0),(4748,0,'!',0),(9773,0,'!',0),(15019,0,'!',0),(15154,0,'!',0),(25213,0,'!',0),(73634,0\n"
     ]
    }
   ],
   "source": [
    "line = line[31:]\n",
    "print(line[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefd6845",
   "metadata": {},
   "source": [
    "## Line processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cec404",
   "metadata": {},
   "source": [
    "We define a function to process individual lines of the SQL file. The schema of the table created by the SQL statement contained in the file can be found at https://www.mediawiki.org/wiki/Manual:Pagelinks_table. The fields that are of interest to us are pl_from (0), pl_namespace (1), pl_title (2), pl_from_namespace (3). To obtain a file containing only proper articles, we eliminate the pages for which the pl_namespace and pl_from_namespace are different from 0 (i.e, they are not in the article namespace). We store the page_id and the page_title in a dictionary defined outside the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a09b4ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_line(line):\n",
    "    n = len(line)\n",
    "    i = 0\n",
    "    while i < n:\n",
    "        if line[i] == \"(\":\n",
    "            start = i + 1\n",
    "        else:\n",
    "            raise Exception(f\"Something went wrong {first_line[i-10:i+10]}\")\n",
    "        while not(line[i] == \")\" and line[i + 1] == \",\" and line[i + 2] == \"(\"):\n",
    "            i += 1\n",
    "            if i == n - 2:\n",
    "                i += 2\n",
    "                break\n",
    "        end = i\n",
    "        i += 2\n",
    "        block = line[start:end]\n",
    "        block = block.split(\",\")\n",
    "        if len(block) >= 4:\n",
    "            if block[1] != '0' or block[3] != '0':\n",
    "                continue\n",
    "            page_id = block[0]\n",
    "            title = block[2]\n",
    "            if title in g:\n",
    "                g[title].append(page_id)\n",
    "            else:\n",
    "                g[title] = [page_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b68c2e8",
   "metadata": {},
   "source": [
    "## Parsing the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28137e82",
   "metadata": {},
   "source": [
    "Using the previously defined function, we want to parse the entire file line by line, store the relevant information in a dictionary and write it to the output text file. To avoid memory overflow, we will write evey 100 titles in the file and than reinitialize the data structure with an empty dictionary. Furthermore, we eliminate pages that are disambiguations, as these are not proper articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ffad446",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = dict()\n",
    "i = 0\n",
    "j = 0\n",
    "output = open(\"article_links.txt\", \"w\")\n",
    "with open(\"/Users/mateicosa/Downloads/enwiki-latest-pagelinks.sql\", \"r\") as f:\n",
    "    line = \"\"\n",
    "    while \"INSERT INTO\" not in line:\n",
    "        line = f.readline()\n",
    "    line = line[31:]\n",
    "    process_line(line)\n",
    "    line = \"INSERT INTO\"\n",
    "    while line:\n",
    "        i += 1\n",
    "        j += 1\n",
    "        line = f.readline()\n",
    "        if \"INSERT INTO\" not in line:\n",
    "            break\n",
    "        line = line[31:]\n",
    "        process_line(line)\n",
    "        if i == 100:\n",
    "            i = 0\n",
    "            for title in g:\n",
    "                if \"(disambiguation)\" not in title:\n",
    "                    output.write(title + \" \" + ' '.join(g[title]) + \"\\n\")\n",
    "            g = dict()\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0478be6",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de2d741",
   "metadata": {},
   "source": [
    "We are left with a file that contains rows having the following structure: page_title page_id_1 page_id_2 ... page_id_n, where page_id_i represents the id of the ith page that contains the link to the current page. We will later use this file together with another one containg the pages titles and ids in order to produce the graph."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
